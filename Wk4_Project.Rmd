---
title: "How well do you lift weights?"
output:
  output=github_document
---

Wearable fitness trackers (e.g., Jawbone Up, Nike FuelBand, and Fitbit) are very popular pieces of tech. They record the duration, distance, and vigor of exercise through the use of accelerometers and other position sensors. Many users use these data to quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

The goal of this project was to predict the manner in which test subjects lifted the barbell. The "manner" of the lift is a factor variable within the "classe" variable in the training set. The dataset consists of x, y, z acceleration, the rate of roll, pitch, and yaw are included. In this study six participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Accelerometers were attached to a belt worn by the participants, the forearm and arm of the participants, and dumbbell that was lifted.

After data cleaning and some quick exploration, the manner/"classe" of the lift three methods were compared: 1) decision tree, 2) random forest, 3) gradient boosted model.

Random forest was found to have the best accuracy and leverage many of the same features as the more simple decision tree. The random forest model is used to predict the blind test dataset.

Below is the analysis:

1) Load Libraries - prediction and visualization packages.
```{r, warning=FALSE, message=FALSE}
library(caret)
library(dplyr)
library(ggplot2)
library(corrr)
library(rattle)
library(randomForest)
```
 
2) Load data
```{r}
training <- read.csv("~/R_data/Coursera/PracticalMachineLearning/pml-training.csv")
testing <-  read.csv("~/R_data/Coursera/PracticalMachineLearning/pml-testing.csv")

#set the classe variable that we are predicting to a factor. It does not exist in the testing dataset,so we only need to do this to the training data.
training$classe <- as.factor(training$classe)
```

3) Data cleaning - we are going to perform some near-zero variance cleaning and removing columns that are >95% NA values. We will be doing these procedures on the training data, which will find the columns we need to remove, then removing those columns from the testing data.
Remove some columns with near-zero variance
```{r}
NZV_index <- nearZeroVar(training)
training <- training[,-NZV_index]
testing <- testing[,-NZV_index]

print(paste(length(NZV_index),"columns removed due to near-zero variance"))
```

Remove columns that are >95% NA 
```{r}
NA_index <- apply(training,2,function(x) sum(is.na(x)))>19000
training <- training[,NA_index==FALSE]
testing <- testing[,NA_index==FALSE]

print(paste(length(NA_index),"columns removed due to being mainly NA values (>95% NA's)"))
```

Remove some columns that are of no use to us in prediction (e.g., user number)
```{r}
training <- training %>%
  select(-X,-user_name,-raw_timestamp_part_1,-raw_timestamp_part_2,-cvtd_timestamp,-num_window)

testing <- testing %>%
  select(-X,-user_name,-raw_timestamp_part_1,-raw_timestamp_part_2,-cvtd_timestamp,-num_window)
```

4) Splitting the training data into training and cross validation.
The training data is quite large (currently 19622 rows). Because of this I decided to split the training data into training (tr_tr) and cross validation  datasets (tr_cv) using a simple data partition on the "classe" variable.  Because there are a lot of observations a simple partition will work fine. If there was less data, using a k-fold or leave-one-out method would be more applicable to conserve as many observations to train and test on.
```{r}
inTrain <- createDataPartition(training$classe, p=0.6)[[1]]
tr_tr <- training[inTrain,]
tr_cv <- training[-inTrain,]
```

5) Exploratory data analysis
The first thing I did to explore the data was to check is whether the feature that we are predicting (classe = A:E) is balanced across our training dataset (tr_tr). From a histogram/frequency plot, we can see that the classes of "classe" have equal observations. This means we are not going to over predict one class because we have more observations of it.
```{r}
plot(tr_tr$classe, main="Levels of the variable classe", xlab="classe levels", ylab="Frequency")
```

Next, I looked at the correlation of the different features. We can see that there are some pretty strongly correlated features. Acceleration of the belt in the y direction (accel_belt_y) is highly correlated with the roll of the belt (roll_belt). This makes sense as these two measurements are very similar. Going forward, if we run into issues with the model's accuracy, or if we feel we are over tuning the model, we could remove/combine some of the features. 
```{r}
tr_cor <- correlate(tr_tr[,1:52]) 
tr_cor %>% shave()
tr_cor %>% rplot()
```

6) Decision tree modeling.
The first model used here is a Decision Model (caret package) using default settings to start. From the confusion matrix the model does pretty poorly at classifying the "classe" variable.
```{r}
modDT <- train(classe~.,data=tr_tr, method="rpart")
predDT <- predict(modDT, newdata=tr_cv)

cmDT <- confusionMatrix(predDT,tr_cv$classe)
cmDT$table
```

Looking at the structure of the decision tree we can see that belt_roll is the strongest feature/root node. 
```{r}
fancyRpartPlot(modDT$finalModel,sub="")
```

7) Random forest modeling.
The second approach used here is a Random Forest Model composed of 100 trees (caret package). From the confusion matrix it's very apparent that this model is more accurate than the decision tree model. 
```{r}
modRF <- train(classe~.,data=tr_tr, method="rf", ntree=100)
predRF <- predict(modRF, newdata=tr_cv)

cmRF <- confusionMatrix(predRF,tr_cv$classe)
cmRF$table
```

Diving into the random forest model, its interesting that the model is using many of the same parameters as the decision tree model above, but is much more accurate.  
```{r}
varImpPlot(modRF$finalModel, main ='Feature importance')
```

8) Gradient boosted model.
The last approach used here is a Gradient boosted model (caret). The confusion matrix shows that there is not a large (if any) increase in the accuracy of the gradient boosted model over.
```{r}
modGB <- train(classe~.,data=tr_tr, method="gbm", verbose=FALSE)
predGB <- predict(modGB, newdata=tr_cv)

cmGB <- confusionMatrix(predGB,tr_cv$classe)
cmGB$table
```

9) Compile the accuracy measures from the confusion matrices. From this we can see that the random forest model is the most accurate. 
```{r}
accTable <- rbind(cmDT$overall[1:4],cmRF$overall[1:4],cmGB$overall[1:4])
row.names(accTable) <- c("Decision Tree","Random Forest","Gradient Boots")
print(accTable)
```

10) Apply the random forest model to blind test dataset and predict the categories of "classe".
```{r}
predTestRF <- predict(modRF, newdata=testing)
print(predTestRF)
```


